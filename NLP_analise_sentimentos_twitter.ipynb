{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589e050a",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos de comentários no Twitter | Transformações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94ef3e",
   "metadata": {},
   "source": [
    "**Giovano Panatta**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5686d7",
   "metadata": {},
   "source": [
    "Uma stopword é uma palavra frequentemente removida de textos no processamento de linguagem natural por ser comum e adicionar pouco significado. Exemplos em português incluem: e, o, a, de, que."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0b90a",
   "metadata": {},
   "source": [
    "**Vamos treinar um modelo de análise de sentimentos usando o conjunto de dados do Twitter, aplicando a técnica de bag-of-words. O objetivo será analisar o sentimento expresso pelos usuários no conjunto de validação. Escolheremos um modelo de aprendizado de máquina adequado para esta tarefa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9f47a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Importando a biblioteca para expressões regulares (regex)\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d764e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento ddos dados\n",
    "train_df = pd.read_csv('twitter_training.csv', header=None) #Não contém nomes nas colunas\n",
    "validation_df = pd.read_csv('twitter_validation.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "941cfc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1         2  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                   3  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "197eb5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1           2  \\\n",
       "0  3364   Facebook  Irrelevant   \n",
       "1   352     Amazon     Neutral   \n",
       "2  8312  Microsoft    Negative   \n",
       "3  4371      CS-GO    Negative   \n",
       "4  4433     Google     Neutral   \n",
       "\n",
       "                                                   3  \n",
       "0  I mentioned on Facebook that I was struggling ...  \n",
       "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       "2  @Microsoft Why do I pay for WORD when it funct...  \n",
       "3  CSGO matchmaking is so full of closet hacking,...  \n",
       "4  Now the President is slapping Americans in the...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fff1278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74682, 4), (1000, 4))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando a estrutura dos dados\n",
    "train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24b59009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3    686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando se há dados nulos em train_df\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "560079f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando se há dados nulos em validation_df\n",
    "validation_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27df52d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Excluindo valores nulos de train_df\n",
    "train_df.dropna(inplace=True)\n",
    "train_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079b55e",
   "metadata": {},
   "source": [
    "**Obs:** Devido a natureza dos nossos dados faltantes (texto), seria difícil tratá-los com métodos estatísticos comuns. E, lavando em consideração que possuímos uma quantidade substancial de dados válidos, a exclusão dos faltantes pareceu ser um caminho válido>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c430427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas duplicadas no train_df: True\n",
      "Linhas duplicadas no validation_df: False\n"
     ]
    }
   ],
   "source": [
    "# Verificando se há linhas duplicadas\n",
    "duplicates_train = train_df.duplicated().any()\n",
    "\n",
    "\n",
    "duplicates_validation = validation_df.duplicated().any()\n",
    "\n",
    "print(\"Linhas duplicadas no train_df:\", duplicates_train)\n",
    "print(\"Linhas duplicadas no validation_df:\", duplicates_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a25f057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando duplicadas\n",
    "num_duplicates_train = train_df.duplicated().sum()\n",
    "num_duplicates_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4c78f",
   "metadata": {},
   "source": [
    "**Obs:** Conforme podemos observar, em nosso dataset de treino, além de entradas NaN ainda contamos com 2340 linhas duplicadas. Desta forma, para que consigamos resulados mais realistas em nossos modelos de ML, teremos também que excluir esses dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd871c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exluindo linhas duplicadas\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "duplicates_train = train_df.duplicated().any()\n",
    "duplicates_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d15b5485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((71656, 4), (1000, 4))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando novamente a estrutura do DF\n",
    "train_df.shape, validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f8f05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.95      0.94      0.94       172\n",
      "    Negative       0.93      0.95      0.94       266\n",
      "     Neutral       0.97      0.95      0.96       285\n",
      "    Positive       0.94      0.94      0.94       277\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.95      0.95      0.95      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Função para pré-processar os textos\n",
    "def preprocess_text(text):\n",
    "    # com regex: substituindo URL's por strings vazias\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE) # ^ passa a corresponder também ao início de cada linha dentro da string.\n",
    "                                                                            # $ passa a corresponder ao fim de cada linha.\n",
    "    # Removendo caracteres especiais e números\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    # Removendo espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I) #faz com que a expressão regular ignore a diferença entre letras maiúsculas e minúsculas\n",
    "    # Convertendo para Minúsculas\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Aplicando o prép rocessamento na coluna 3 do dataset\n",
    "train_df[3] = train_df[3].astype(str).apply(preprocess_text)\n",
    "validation_df[3] = validation_df[3].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Convertendo os textos dos tweets em uma matriz bag-of-words com stop_words em inglês\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_counts = vectorizer.fit_transform(train_df[3])\n",
    "X_validation_counts = vectorizer.transform(validation_df[3])\n",
    "\n",
    "# Preparando as variáveis alvo sendo a coluna 2 (irrelevant, negative, neutral ou positive)\n",
    "y_train = train_df[2]\n",
    "y_validation = validation_df[2]\n",
    "\n",
    "# Inicializando o modelo de Regressão Logística com 1000 iterações (outros testes realizados, este performou melhor)\n",
    "log_reg_classifier = LogisticRegression(max_iter=1000, tol=0.1)\n",
    "\n",
    "# Treinando o modelo\n",
    "log_reg_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred_log_reg = log_reg_classifier.predict(X_validation_counts)\n",
    "print(classification_report(y_validation, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcde929",
   "metadata": {},
   "source": [
    "**Vamos usar o mesmo conjunto de dados do Twitter da questão anterior, mas agora aplicaremos a técnica bag-of-ngrams para treinar um modelo de análise de sentimentos. Compararemos o desempenho de n-grams com duas e três palavras para ver qual oferece melhores insights sobre os sentimentos dos usuários. Esse processo nos ajudará a entender o impacto do tamanho dos n-grams na precisão da análise de sentimentos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31eabf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Report:\n",
      "              precision    recall  f1-score  support\n",
      "Irrelevant     1.000000  0.976744  0.988235   172.00\n",
      "Negative       0.980695  0.954887  0.967619   266.00\n",
      "Neutral        0.992780  0.964912  0.978648   285.00\n",
      "Positive       0.922297  0.985560  0.952880   277.00\n",
      "accuracy       0.970000  0.970000  0.970000     0.97\n",
      "macro avg      0.973943  0.970526  0.971845  1000.00\n",
      "weighted avg   0.971283  0.970000  0.970225  1000.00\n",
      "\n",
      "Trigrams Report:\n",
      "              precision    recall  f1-score   support\n",
      "Irrelevant     1.000000  0.918605  0.957576   172.000\n",
      "Negative       0.915385  0.894737  0.904943   266.000\n",
      "Neutral        1.000000  0.929825  0.963636   285.000\n",
      "Positive       0.842271  0.963899  0.898990   277.000\n",
      "accuracy       0.928000  0.928000  0.928000     0.928\n",
      "macro avg      0.939414  0.926766  0.931286  1000.000\n",
      "weighted avg   0.933801  0.928000  0.929074  1000.000\n"
     ]
    }
   ],
   "source": [
    "# Configurando Count vectorizer para bigrams (2,2)\n",
    "vectorizer_bigrams = CountVectorizer(stop_words='english', ngram_range=(2, 2))\n",
    "X_train_bigrams = vectorizer_bigrams.fit_transform(train_df[3])\n",
    "X_validation_bigrams = vectorizer_bigrams.transform(validation_df[3])\n",
    "\n",
    "# Treinando o modelo de Regressão Logística com bigrams com tolerância de convergência 0.1\n",
    "log_reg_classifier_bigrams = LogisticRegression(max_iter=1000, tol=0.1)\n",
    "log_reg_classifier_bigrams.fit(X_train_bigrams, y_train)\n",
    "\n",
    "# Avaliando o modelo no conjunto de validação com bigrams\n",
    "y_pred_bigrams = log_reg_classifier_bigrams.predict(X_validation_bigrams)\n",
    "report_bigrams = classification_report(y_validation, y_pred_bigrams)\n",
    "\n",
    "# Configurando Count vectorizer para trigrams (3,3)\n",
    "vectorizer_trigrams = CountVectorizer(stop_words='english', ngram_range=(3, 3))\n",
    "X_train_trigrams = vectorizer_trigrams.fit_transform(train_df[3])\n",
    "X_validation_trigrams = vectorizer_trigrams.transform(validation_df[3])\n",
    "\n",
    "# Treinando o modelo de Regressão Logistica com trigrams com tolerância de convergência 0.1\n",
    "log_reg_classifier_trigrams = LogisticRegression(max_iter=1000, tol=0.1)\n",
    "log_reg_classifier_trigrams.fit(X_train_trigrams, y_train)\n",
    "\n",
    "# Avaliando o modelo no conjunto de validação com trigrams\n",
    "y_pred_trigrams = log_reg_classifier_trigrams.predict(X_validation_trigrams)\n",
    "report_trigrams = classification_report(y_validation, y_pred_trigrams)\n",
    "\n",
    "report_bigrams_json = classification_report(y_validation, y_pred_bigrams, output_dict=True)\n",
    "report_trigrams_json = classification_report(y_validation, y_pred_trigrams, output_dict=True)\n",
    "\n",
    "#Transformando o report em JSON para organizar a visualização\n",
    "df_bigrams = pd.DataFrame(report_bigrams_json).transpose()\n",
    "df_trigrams = pd.DataFrame(report_trigrams_json).transpose()\n",
    "\n",
    "\n",
    "print(\"Bigrams Report:\")\n",
    "print(df_bigrams)\n",
    "print(\"\\nTrigrams Report:\")\n",
    "print(df_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763caf4",
   "metadata": {},
   "source": [
    "\n",
    "* Modelo com Bigrams: Acurácia de 97%, resultados consistentes e precisos para todas as classes.\n",
    "\n",
    "* Modelo com Trigrams: Acurácia de 93%, resultados ligeiramente inferiores\n",
    "\n",
    "Conclusão: Bigrams superaram os trigrams, oferecendo resultados mais consistentes e precisos para análise de sentimentos do Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18b8af",
   "metadata": {},
   "source": [
    "**Vamos aplicar a técnica TF-IDF ao mesmo conjunto de dados do Twitter para treinar um modelo de análise de sentimentos. O objetivo é verificar e comparar o desempenho desta abordagem com as técnicas anteriores, bag-of-words e bag-of-ngrams. A técnica TF-IDF nos ajudará a entender como a frequência de palavras e a importância delas no contexto do conjunto de dados influenciam a análise dos sentimentos expressos pelos usuários.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7339ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "Irrelevant     0.916667  0.895349  0.905882   172.000\n",
      "Negative       0.881119  0.947368  0.913043   266.000\n",
      "Neutral        0.951673  0.898246  0.924188   285.000\n",
      "Positive       0.916968  0.916968  0.916968   277.000\n",
      "accuracy       0.916000  0.916000  0.916000     0.916\n",
      "macro avg      0.916606  0.914483  0.915020  1000.000\n",
      "weighted avg   0.917271  0.916000  0.916075  1000.000\n"
     ]
    }
   ],
   "source": [
    "# Configurando o TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df[3].astype(str)) # Convertendo para string para garantir compatibilidade\n",
    "X_validation_tfidf = tfidf_vectorizer.transform(validation_df[3].astype(str))\n",
    "\n",
    "# Treinando o modelo de Regressão Logística com 1000 iterações e tolerância de convergência 0.1\n",
    "log_reg_classifier_tfidf = LogisticRegression(max_iter=1000, tol=0.1) \n",
    "log_reg_classifier_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Avaliando o modelo com o conjunto de validação ()\n",
    "y_pred_tfidf = log_reg_classifier_tfidf.predict(X_validation_tfidf)\n",
    "\n",
    "# Gerando e formatando o relatório em DF para facilitar a visualização\n",
    "report_dict = classification_report(y_validation, y_pred_tfidf, output_dict=True)\n",
    "df_report = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# Imprimindo o DataFrame para visualização clara\n",
    "print(df_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae05510",
   "metadata": {},
   "source": [
    "**Conclusão**: Com a acurácia, o modelo previu corretamente a classificação de 91.6% das amostras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd60ae",
   "metadata": {},
   "source": [
    "**Vamos trabalhar com o dataset Breast Cancer Wisconsin, disponível através do sklearn.datasets.load_breast_cancer. O processo incluirá a transformação dos dados utilizando técnicas como MinMaxScaler e StandardScaler. Além disso, aplicaremos pelo menos mais duas técnicas de transformação. Com os dados transformados, utilizaremos um modelo de regressão logística para avaliar o desempenho em termos de precisão, recall, e outras métricas relevantes.**\n",
    "\n",
    "**Este processo nos ajudará a entender como diferentes técnicas de pré-processamento de dados podem influenciar a performance de modelos de aprendizado de máquina, especialmente em tarefas de classificação como a detecção de câncer de mama.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eaa71b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Carregando o conjunto.\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Criando o dataframe com as colunas como features\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Adicionando a variável alvo 'target' ao DF\n",
    "df['target'] = data.target\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c27aad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0a83f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "target                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d39caa4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated = df.duplicated().any()\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cde5251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MinMaxScaler': 0.9649122807017544,\n",
       " 'StandardScaler': 0.9824561403508771,\n",
       " 'RobustScaler': 0.9883040935672515,\n",
       " 'PowerTransformer': 0.9824561403508771}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Atribuindo os dados a X features e y 'target'\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Dividindo o dataset em conjuntos de treino 70% e teste 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Iniciando as transformações (aplicando em um dicionário)\n",
    "scalers = {\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "    \"PowerTransformer\": PowerTransformer()\n",
    "}\n",
    "\n",
    "# Avaliando o modelo com cada transformador\n",
    "results = {}\n",
    "#scaler name = a chave do dicionário de transformadores, scaler = o valor\n",
    "for scaler_name, scaler in scalers.items(): #utilizando o dicionário (tuplas) que contem as transformações\n",
    "    # Aplicando a transformação\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Treinando o modelo de regressão logística com 1000 iterações\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Avaliando o modelo\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True) #output_dict=True (retorna como dicionário)\n",
    "    results[scaler_name] = report['accuracy']\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8854bff",
   "metadata": {},
   "source": [
    "**Conclusão:** As técnicas de transformação de dados melhoraram a acurácia do modelo de regressão logística no dataset Breast Cancer Wisconsin, com o RobustScaler alcançando a melhor acurácia (98.83%), mostrando a eficácia de adaptar a transformação às características do dataset, especialmente na presença de outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
